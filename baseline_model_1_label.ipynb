{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cisco - Ariel University API Security Detection Challenge 2023\n",
    "## Lable 1 code\n",
    "\n",
    "\n",
    "### Imports and global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, settings and first dataset view\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Set pandas to show all columns when you print a dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "dataset_number = 1\n",
    "test_type = 'label'\n",
    "\n",
    "# Read the json and read it to a pandas dataframe object\n",
    "with open(f'./datasets/dataset_{str(dataset_number)}_train.json') as file:\n",
    "    raw_ds = json.load(file)\n",
    "df = pd.json_normalize(raw_ds, max_level=2)\n",
    "\n",
    "# Shoe the first five lines of the dataframe to see if everything was read accordingly \n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic dataset label arrangements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "# Extracts the path from the url\n",
    "def url_path(row):\n",
    "    parsed = urlparse(row[\"request.url\"])\n",
    "    return parsed.path\n",
    "\n",
    "# Extracts the queries from the url\n",
    "def url_query(row):\n",
    "    parsed = urlparse(row[\"request.url\"])\n",
    "    return parsed.query\n",
    "\n",
    "# Extracts the length of the url\n",
    "def url_len(row):\n",
    "    return len(row[\"request.url\"])\n",
    "\n",
    "# Checks if any of the suspected headers contains a dollar sign\n",
    "def has_dollarsign(row):\n",
    "    if \"$\" in row[\"request.headers.Accept-Encoding\"]:\n",
    "        return True\n",
    "    elif \"$\" in row[\"request.headers.Sec-Fetch-Site\"]:\n",
    "        return True\n",
    "    elif \"$\" in row[\"request.headers.Sec-Fetch-Dest\"]:\n",
    "        return True\n",
    "    elif \"$\" in row[\"request.headers.Set-Cookie\"]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the black attack tag lines with \"Benign\" string\n",
    "df['request.Attack_Tag'] = df['request.Attack_Tag'].fillna('Benign')\n",
    "df['attack_type'] = df['request.Attack_Tag']\n",
    "\n",
    "# This function will be used in the lambda below to iterate over the label columns \n",
    "def categorize(row):  \n",
    "    if row['request.Attack_Tag'] == 'Benign':\n",
    "        return 'Benign'\n",
    "    return 'Malware'\n",
    "\n",
    "df['label'] = df.apply(lambda row: categorize(row), axis=1)\n",
    "\n",
    "# make new columns for the features of: url length, url path, url query, and if the suspected headers contains a dollar sign\n",
    "df['url_length'] = df.apply(lambda row: url_len(row), axis=1)\n",
    "df[\"url_path\"] = df.apply(lambda row: url_path(row), axis=1)\n",
    "df[\"url_query\"] = df.apply(lambda row: url_query(row), axis=1)\n",
    "df[\"has_dollarsign\"] = df.apply(lambda row: has_dollarsign(row), axis=1)\n",
    "\n",
    "# After finishing the arrangements we delete the irrelevant column\n",
    "df.drop('request.Attack_Tag', axis=1, inplace=True)\n",
    "df.drop('request.url', axis=1, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all NAN columns or replace with desired string\n",
    "# This loop iterates over all of the column names which are all NaN\n",
    "for column in df.columns[df.isna().any()].tolist():\n",
    "    df[column] = df[column].fillna('None')\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a list of the headers that we want to remove from our model\n",
    "COLUMNS_TO_REMOVE = ['request.body',\n",
    "                    'response.headers.Content-Length',\n",
    "                    'request.headers.Date', \n",
    "                    \"request.headers.Sec-Fetch-Dest\", \n",
    "                    \"request.headers.Connection\", \n",
    "                    \"request.headers.Accept\", \n",
    "                    \"request.headers.Sec-Fetch-User\",\n",
    "                    \"request.headers.Host\",\n",
    "                    \"response.headers.Set-Cookie\", \n",
    "                    \"response.status\"]\n",
    "\n",
    "# This is our main preprocessing function that will iterate over all of the chosen \n",
    "# columns and run some feature extraction models\n",
    "def vectorize_df(df):\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Run LabelEncoder on the chosen features\n",
    "    for column in df.columns.to_list(): \n",
    "        df[column] = le.fit_transform(df[column])\n",
    "    \n",
    "    # Remove some columns that may be needed for the model\n",
    "    for column in COLUMNS_TO_REMOVE: \n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df = vectorize_df(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory check (For large datasets sometimes the dataframe will exceed the computers resources)\n",
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the right features\n",
    "# In our example code we choose all the columns as our feature\n",
    "\n",
    "features_list = df.columns.to_list()\n",
    "features_list.remove('label')\n",
    "features_list.remove('attack_type')\n",
    "print(features_list)\n",
    "\n",
    "# Recheck all datatype before training to see we don't have any objects in our features\n",
    "# In this example our model must get features containing only numbers so we recheck to see if we missed anything during preprocessing\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data train and test split preparations. Here we will insert our feature list and label list.\n",
    "# Afterwards the data will be trained and fitted on the amazing RandomForest model\n",
    "# X_Train and y_Train will be used for training\n",
    "# X_test and y_test will be used for over fitting checking and overall score testing\n",
    "\n",
    "# We convert the feature list to a numpy array, this is required for the model fitting\n",
    "X = df[features_list].to_numpy()\n",
    "\n",
    "# This column is the desired prediction we will train our model on\n",
    "y = np.stack(df[test_type])\n",
    "\n",
    "# We split the dataset to train and test according to the required ration\n",
    "# Do not change the test_size -> you can change anything else\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1765, random_state=42, stratify=y)\n",
    "\n",
    "# We print the resulted datasets and count the difference \n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "counter = Counter(y)\n",
    "counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model choosing and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train the model on the train dataset\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Check data balance and variety\n",
    "print(sorted(Counter(y_train).items()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print our results\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "true_labels = y_test\n",
    "clf_matrix = confusion_matrix(true_labels, predictions)\n",
    "clf_report = classification_report(true_labels, predictions, digits=5)\n",
    "heatmap = sns.heatmap(clf_matrix, annot=True, cmap='Blues', fmt='g', \n",
    "                      xticklabels=np.unique(true_labels), \n",
    "                      yticklabels=np.unique(true_labels)) \n",
    "\n",
    "# The heatmap is cool but this is the most important result\n",
    "print(clf_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the valuation json, preprocess it and run the model \n",
    "with open(f'./datasets/dataset_{str(dataset_number)}_val.json') as file:\n",
    "    raw_ds = json.load(file)\n",
    "test_df = pd.json_normalize(raw_ds, max_level=2)\n",
    "\n",
    "# Preprocess the validation dataset\n",
    "test_df['url_length'] = test_df.apply(lambda row: url_len(row), axis=1)\n",
    "test_df[\"url_path\"] = test_df.apply(lambda row: url_path(row), axis=1)\n",
    "test_df[\"url_query\"] = test_df.apply(lambda row: url_query(row), axis=1)\n",
    "test_df[\"has_dollarsign\"] = test_df.apply(lambda row: has_dollarsign(row), axis=1)\n",
    "for column in test_df.columns[test_df.isna().any()].tolist():\n",
    "    test_df[column] = test_df[column].fillna('None')\n",
    "test_df = vectorize_df(test_df)\n",
    "\n",
    "# Predict with the model\n",
    "X = test_df[features_list].to_numpy()\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preditions\n",
    "enc = LabelEncoder()\n",
    "np.savetxt(f'./datasets/dataset_{str(dataset_number)}_{test_type}_result.txt', enc.fit_transform(predictions), fmt='%2d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
